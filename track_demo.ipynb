{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking pipeline <br>\n",
    "This notebook demonstrates steps required to bee build trajectories, from bee detections (for those we have [another tutorial](https://github.com/oist/DenseObjectDetection)), through trajectory construction, to generation of videos of tracked bees.\n",
    "\n",
    "### Requirements\n",
    "* Python 3.5+\n",
    "* [TensorFlow](https://www.tensorflow.org/) (1.9 or higher)\n",
    "* [Numpy](http://www.numpy.org/)\n",
    "* [Pillow](https://pillow.readthedocs.io/en/stable/)\n",
    "* [matplotlib](https://matplotlib.org/)\n",
    "\n",
    "\n",
    "First we will download and unpack sample data for this notebook (depending on your connection this might take a moment):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, tarfile, os\n",
    "import utils\n",
    "\n",
    "fname = \"sample_data.tgz\"\n",
    "progress = utils.DownloadProgress()\n",
    "urllib.request.urlretrieve(\"https://beepositions.unit.oist.jp/\" + fname, fname, reporthook=progress.progress_hook)\n",
    "\n",
    "with tarfile.open(fname, \"r:gz\") as tar:\n",
    "    tar.extractall()\n",
    "\n",
    "os.remove(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of the data folder is:<br>\n",
    "```\n",
    "sample_data\n",
    "+-- checkpoints  \n",
    "|   +-- model_005000.ckpt.*\n",
    "+-- detections  \n",
    "|   +-- *.txt\n",
    "+-- frames  \n",
    "|   +-- *.png\n",
    "```\n",
    "`frames` folder contains following in time frames from a video of a beehive. `detection` folder contains detections of bees in the respective frames, with text format `x,y,class,angle` for each detection. <br>\n",
    "We will plot detections to inspect their accuracy. We can plot individual frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import Image, display\n",
    "import plot\n",
    "\n",
    "fr = 10\n",
    "plot.plot_detections(fr, save=True)\n",
    "display(Image(os.path.join(utils.PLOTS_DIR, \"%06d.png\" % fr), unconfined=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is 100 frames in this sample dataset, we can merge them into an animation. For convenience, here instead of vidoes we will generate animated gif files. The files will be stored in the data folder and displayed here below. The animation is played in a repeated loop.<br>\n",
    "To generate the animation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.plot_detection_video()\n",
    "\n",
    "display(Image(os.path.join(utils.PLOTS_DIR, \"detections.gif\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the trajectory construction procedure we exploit not only position and postural information that is included in the `detections` folder, but also numeric *embeddings* encoding visual aspects of each individual bee. The embeddings are derived from a pretrained neural network.<br>\n",
    "You can build the embeddings with the `build_embeddings` function in the `embed` module. This function will read the image and detection data, crop the detections from the images and feed into the network. The output of the network is then saved in text and binary files in the `detections_embeddings` subfolder of the data folder.<br>\n",
    "To run the procedure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import embed\n",
    "embed.build_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the created files the position and posture information `x,y,class,angle` of each detection is concatenated with another 64 numbers forming the embedding of the detection visual features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "fr = 10\n",
    "det = np.load(os.path.join(utils.FTS_DIR, \"%06d.npy\" % fr))\n",
    "print(\"frame %i - detections: %i, embedding size: %i\" % (fr, det.shape[0], det.shape[1]-4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the embeddings we will now perform trajectory construction procedure. The logic and parameters of this procedure are described in our manuscript and included in the `track` module. We will first perform the matching `build_trajectories()`, then sort them according to their length `sort_trajectories`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import track\n",
    "\n",
    "track.build_trajectories()\n",
    "track.sort_trajectories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generated trajectories are located in the `trajectories` subfolder of the data folder. In addition to trajectory files, file `tra_lens.txt` lists starting point and length of each trajectory. Using this file can easily inspect the lengths of the generated trajectories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tra_lens = np.loadtxt(os.path.join(utils.TRACK_DIR, \"tra_lens.txt\"), delimiter=\",\", dtype=np.int)\n",
    "_ = plt.hist(tra_lens[:,1], bins=50)\n",
    "_ = plt.xlabel('Trajectory lengths')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can generate animations of some example trajectories. All files will be stored in the `plots` subfolder of the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import Image, display\n",
    "import plot, utils\n",
    "\n",
    "trajectory = 28 # change this number to plot any other generated trajectory\n",
    "trajectory_filename = plot.plot_trajectory(trajectory)\n",
    "display(Image(trajectory_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also generate a video of all generated trajectories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trajectories_filename = plot.plot_all_trajectories()\n",
    "display(Image(all_trajectories_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
